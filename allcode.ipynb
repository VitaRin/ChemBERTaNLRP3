{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas transformers datasets sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Existing molecule processing:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv('CHembl training set.csv')\n",
    "df = df[['Smiles', 'Comment', 'IC50']]\n",
    "\n",
    "def categorize_activity(value):\n",
    "    if value == \"very active\":\n",
    "        return 2\n",
    "    elif value == \"moderately active\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['label'] = df['Comment'].apply(categorize_activity)\n",
    "\n",
    "df.rename(columns={'Smiles': 'text'}, inplace=True)\n",
    "df.drop(columns=['Comment'], inplace=True)\n",
    "\n",
    "\n",
    "if not os.path.exists(f'./traindata'):\n",
    "    os.makedirs(f'./traindata')\n",
    "\n",
    "df.to_csv('traindata/train_with_IC50.csv', index=False)\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df['label'].value_counts()[2])\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "df.to_csv('traindata/train_withduplicates.csv', index=False)\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "print(df['label'].value_counts()[2])\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "df.to_csv('traindata/train_shuffled.csv', index=False)\n",
    "\n",
    "# Method for dividing the data into three datasets for training, testing, and validation\n",
    "def split_dataset(df):\n",
    "    train_samples, test_samples, val_samples = [], [], []\n",
    "    very_active_samples = df[df['label'] == 2].values.tolist()\n",
    "    moderately_active_samples = df[df['label'] == 1].values.tolist()\n",
    "    inactive_samples = df[df['label'] == 0].values.tolist()\n",
    "\n",
    "    random.shuffle(very_active_samples)\n",
    "    random.shuffle(moderately_active_samples)\n",
    "    random.shuffle(inactive_samples)\n",
    "\n",
    "    for samples in [very_active_samples, moderately_active_samples, inactive_samples]:\n",
    "        train_size = int(len(samples) * 0.8)\n",
    "        test_size = int(len(samples) * 0.1) \n",
    "\n",
    "        train_samples.extend(samples[:train_size])\n",
    "        test_samples.extend(samples[train_size:train_size + test_size])\n",
    "        val_samples.extend(samples[train_size + test_size:])\n",
    "\n",
    "    return train_samples, test_samples, val_samples\n",
    "\n",
    "\n",
    "# Split and balance the dataset three times\n",
    "for j in range(2):\n",
    "    for i in range(3):\n",
    "        train_samples, test_samples, val_samples = split_dataset(df)\n",
    "        \n",
    "        if not os.path.exists(f'./traindata{j}'):\n",
    "            os.makedirs(f'./traindata{j}')\n",
    "\n",
    "        # Write the selected samples to separate CSV files\n",
    "        random.shuffle(train_samples)\n",
    "        train_df = pd.DataFrame(train_samples, columns=df.columns)\n",
    "        train_df.to_csv(f'traindata{j}/train_data_{i}.csv', index=False)\n",
    "\n",
    "        print(train_df['label'].value_counts()[2])\n",
    "        print(train_df['label'].value_counts()[1])\n",
    "        print(train_df['label'].value_counts()[0])\n",
    "\n",
    "        random.shuffle(test_samples)\n",
    "        test_df = pd.DataFrame(test_samples, columns=df.columns)\n",
    "        test_df.to_csv(f'traindata{j}/test_data_{i}.csv', index=False)\n",
    "\n",
    "        print(test_df['label'].value_counts()[2])\n",
    "        print(test_df['label'].value_counts()[1])\n",
    "        print(test_df['label'].value_counts()[0])\n",
    "\n",
    "        random.shuffle(val_samples)\n",
    "        val_df = pd.DataFrame(val_samples, columns=df.columns)\n",
    "        val_df.to_csv(f'traindata{j}/val_data_{i}.csv', index=False)\n",
    "\n",
    "        print(val_df['label'].value_counts()[2])\n",
    "        print(val_df['label'].value_counts()[1])\n",
    "        print(val_df['label'].value_counts()[0])\n",
    "\n",
    "\n",
    "def split_final_dataset(df):\n",
    "    # Split the dataset into lists based on the classification labels\n",
    "    very_active_samples = df[df['label'] == 2].values.tolist()\n",
    "    moderately_active_samples = df[df['label'] == 1].values.tolist()\n",
    "    inactive_samples = df[df['label'] == 0].values.tolist()\n",
    "\n",
    "    # Shuffle each list\n",
    "    random.shuffle(very_active_samples)\n",
    "    random.shuffle(moderately_active_samples)\n",
    "    random.shuffle(inactive_samples)\n",
    "\n",
    "    # Create separate lists for training, testing, and validation sets, selecting samples while maintaining balance\n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "\n",
    "    for samples in [very_active_samples, moderately_active_samples, inactive_samples]:\n",
    "        train_size = int(len(samples) * 0.8)\n",
    "\n",
    "        train_samples.extend(samples[:train_size])\n",
    "        val_samples.extend(samples[train_size:])\n",
    "\n",
    "    return train_samples, val_samples\n",
    "\n",
    "\n",
    "# Split and balance the dataset three times\n",
    "train_samples, val_samples = split_final_dataset(df)\n",
    "    \n",
    "# Write the selected samples to separate CSV files\n",
    "random.shuffle(train_samples)\n",
    "train_df = pd.DataFrame(train_samples, columns=df.columns)\n",
    "train_df.to_csv(f'traindata/train_data.csv', index=False)\n",
    "\n",
    "print(train_df['label'].value_counts()[2])\n",
    "print(train_df['label'].value_counts()[1])\n",
    "print(train_df['label'].value_counts()[0])\n",
    "\n",
    "random.shuffle(val_samples)\n",
    "val_df = pd.DataFrame(val_samples, columns=df.columns)\n",
    "val_df.to_csv(f'traindata/val_data.csv', index=False)\n",
    "\n",
    "print(val_df['label'].value_counts()[2])\n",
    "print(val_df['label'].value_counts()[1])\n",
    "print(val_df['label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "df = pd.read_csv('CHembl training set.csv')\n",
    "df = df[['Smiles', 'Comment', 'IC50']]\n",
    "\n",
    "def categorize_activity(value):\n",
    "    if value == \"inactive\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Create the label column from comments\n",
    "df['label'] = df['Comment'].apply(categorize_activity)\n",
    "\n",
    "df.rename(columns={'Smiles': 'text'}, inplace=True)\n",
    "df.drop(columns=['Comment'], inplace=True)\n",
    "\n",
    "df.to_csv('traindata/train2_with_IC50.csv', index=False)\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.to_csv('traindata/train2_withduplicates.csv', index=False)\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
    "\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "df.to_csv('traindata/train2_shuffled.csv', index=False)\n",
    "\n",
    "def split_dataset(df):\n",
    "    # Split the dataset into lists based on the classification labels\n",
    "    active_samples = df[df['label'] == 1].values.tolist()\n",
    "    inactive_samples = df[df['label'] == 0].values.tolist()\n",
    "\n",
    "    # Shuffle each list\n",
    "    random.shuffle(active_samples)\n",
    "    random.shuffle(inactive_samples)\n",
    "\n",
    "    # Create separate lists for training, testing, and validation sets, selecting samples while maintaining balance\n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "\n",
    "    for samples in [active_samples, inactive_samples]:\n",
    "        train_size = int(len(samples) * 0.8)\n",
    "\n",
    "        train_samples.extend(samples[:train_size])\n",
    "        val_samples.extend(samples[train_size:])\n",
    "\n",
    "    return train_samples, val_samples\n",
    "\n",
    "\n",
    "# Split and balance the dataset\n",
    "train_samples, val_samples = split_dataset(df)\n",
    "    \n",
    "# Write the selected samples to separate CSV files\n",
    "random.shuffle(train_samples)\n",
    "train_df = pd.DataFrame(train_samples, columns=df.columns)\n",
    "train_df.to_csv(f'traindata/train_data2.csv', index=False)\n",
    "\n",
    "print(train_df['label'].value_counts()[1])\n",
    "print(train_df['label'].value_counts()[0])\n",
    "\n",
    "random.shuffle(val_samples)\n",
    "val_df = pd.DataFrame(val_samples, columns=df.columns)\n",
    "val_df.to_csv(f'traindata/val_data2.csv', index=False)\n",
    "\n",
    "print(val_df['label'].value_counts()[1])\n",
    "print(val_df['label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Generated molecule processing:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_excel('generatedmol.xlsx')\n",
    "\n",
    "# Remove invalid molecules\n",
    "df = df[df['Upper boundary for estimated affinity [nM]'] != 0]\n",
    "\n",
    "# Define a function to categorize activity\n",
    "def categorize_activity(value):\n",
    "    if value < 250:\n",
    "        return 2\n",
    "    elif 250 <= value <= 5000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# Create the label column based on estimated affinity values\n",
    "df['label'] = df['Upper boundary for estimated affinity [nM]'].apply(categorize_activity)\n",
    "df.rename(columns={'USMILES': 'text'}, inplace=True)\n",
    "\n",
    "df['IC50'] = df['Upper boundary for estimated affinity [nM]']\n",
    "df = df[['text', 'label', 'IC50']]\n",
    "\n",
    "print(df['label'].value_counts()[2])\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
    "\n",
    "if not os.path.exists(f'./testdata'):\n",
    "    os.makedirs(f'./testdata')\n",
    "\n",
    "df.to_csv('testdata/testset_IC50.csv', index=False)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=['IC50'], inplace=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "print(df['label'].value_counts()[2])\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "df.to_csv('testdata/testset_noduplicates.csv', index=False)\n",
    "\n",
    "very_active_samples = df[df['label'] == 2].values.tolist()\n",
    "moderately_active_samples = df[df['label'] == 1].values.tolist()\n",
    "inactive_samples = df[df['label'] == 0].values.tolist()\n",
    "\n",
    "inactive_samples = random.sample(inactive_samples, len(very_active_samples) + len(moderately_active_samples))\n",
    "\n",
    "random.shuffle(very_active_samples)\n",
    "random.shuffle(moderately_active_samples)\n",
    "random.shuffle(inactive_samples)\n",
    "\n",
    "test_samples = []\n",
    "val_samples = []\n",
    "all_samples = []\n",
    "\n",
    "for samples in [very_active_samples, moderately_active_samples, inactive_samples]:\n",
    "    size = int(len(samples) * 0.5)\n",
    "\n",
    "    test_samples.extend(samples[:size])\n",
    "    val_samples.extend(samples[size:])\n",
    "    all_samples.extend(samples)\n",
    "\n",
    "\n",
    "random.shuffle(all_samples)\n",
    "fullset_df = pd.DataFrame(all_samples, columns= df.columns)\n",
    "fullset_df.to_csv('testdata/all_synthesized.csv', index=False)\n",
    "print(fullset_df['label'].value_counts()[2])\n",
    "print(fullset_df['label'].value_counts()[1])\n",
    "print(fullset_df['label'].value_counts()[0])\n",
    "\n",
    "random.shuffle(test_samples)\n",
    "test_df = pd.DataFrame(test_samples, columns=df.columns)\n",
    "test_df.to_csv('testdata/testdata.csv', index=False)\n",
    "\n",
    "print(test_df['label'].value_counts()[2])\n",
    "print(test_df['label'].value_counts()[1])\n",
    "print(test_df['label'].value_counts()[0])\n",
    "\n",
    "random.shuffle(val_samples)\n",
    "val_df = pd.DataFrame(val_samples, columns=df.columns)\n",
    "val_df.to_csv('testdata/valdata.csv', index=False)\n",
    "\n",
    "print(val_df['label'].value_counts()[2])\n",
    "print(val_df['label'].value_counts()[1])\n",
    "print(val_df['label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "df = pd.read_excel('generatedmol.xlsx')\n",
    "\n",
    "# Remove invalid molecules\n",
    "df = df[df['Upper boundary for estimated affinity [nM]'] != 0]\n",
    "\n",
    "# Define a function to categorize activity\n",
    "def categorize_activity(value):\n",
    "    if value >= 5000:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# Create the label column based on estimated affinity values\n",
    "df['label'] = df['Upper boundary for estimated affinity [nM]'].apply(categorize_activity)\n",
    "\n",
    "df.rename(columns={'USMILES': 'text'}, inplace=True)\n",
    "\n",
    "df['IC50'] = df['Upper boundary for estimated affinity [nM]']\n",
    "df = df[['text', 'label', 'IC50']]\n",
    "\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
    "df.to_csv('testdata/testset2_IC50.csv', index=False)\n",
    "\n",
    "# Drop unneccessary columns\n",
    "df.drop(columns=['IC50'], inplace=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "print(df['label'].value_counts()[1])\n",
    "print(df['label'].value_counts()[0])\n",
    "\n",
    "# Save the modified DataFrame back to a csv file\n",
    "df.to_csv('testdata/testset2_noduplicates.csv', index=False)\n",
    "\n",
    "active_samples = df[df['label'] == 1].values.tolist()\n",
    "inactive_samples = df[df['label'] == 0].values.tolist()\n",
    "\n",
    "inactive_samples = random.sample(inactive_samples, len(active_samples))\n",
    "\n",
    "random.shuffle(active_samples)\n",
    "random.shuffle(inactive_samples)\n",
    "\n",
    "test_samples = []\n",
    "val_samples = []\n",
    "all_samples = []\n",
    "\n",
    "for samples in [active_samples, inactive_samples]:\n",
    "    size = int(len(samples) * 0.5)\n",
    "\n",
    "    test_samples.extend(samples[:size])\n",
    "    val_samples.extend(samples[size:])\n",
    "    all_samples.extend(samples)\n",
    "\n",
    "\n",
    "random.shuffle(all_samples)\n",
    "fullset_df = pd.DataFrame(all_samples, columns= df.columns)\n",
    "fullset_df.to_csv('testdata/all_synthesized2.csv', index=False)\n",
    "\n",
    "print(fullset_df['label'].value_counts()[1])\n",
    "print(fullset_df['label'].value_counts()[0])\n",
    "\n",
    "random.shuffle(test_samples)\n",
    "test_df = pd.DataFrame(test_samples, columns=df.columns)\n",
    "test_df.to_csv('testdata/testdata2.csv', index=False)\n",
    "\n",
    "print(test_df['label'].value_counts()[1])\n",
    "print(test_df['label'].value_counts()[0])\n",
    "\n",
    "random.shuffle(val_samples)\n",
    "val_df = pd.DataFrame(val_samples, columns=df.columns)\n",
    "val_df.to_csv('testdata/valdata2.csv', index=False)\n",
    "\n",
    "print(val_df['label'].value_counts()[1])\n",
    "print(val_df['label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Confirmed generated inhibitor data processing:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel('goodmol.xlsx')\n",
    "\n",
    "# Create the label column based on estimated affinity values\n",
    "def categorize_activity(value):\n",
    "    if value < 250:\n",
    "        return 2\n",
    "    elif 250 <= value <= 5000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Convert the labels\n",
    "df['label'] = df['Upper boundary for estimated affinity [nM]'].apply(categorize_activity)\n",
    "df.rename(columns={'USMILES': 'text'}, inplace=True)\n",
    "\n",
    "df['IC50'] = df['Upper boundary for estimated affinity [nM]']\n",
    "df = df[['text', 'label', 'IC50']]\n",
    "\n",
    "df.to_csv('mols_IC50.csv', index=False)\n",
    "\n",
    "# Drop unneccessary columns\n",
    "df.drop(columns=['IC50'], inplace=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "# Save the modified DataFrame back to a csv file\n",
    "df.to_csv('mols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_excel('goodmol.xlsx')\n",
    "\n",
    "# Define a function to categorize activity\n",
    "def categorize_activity(value):\n",
    "    if value >= 5000:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Create the label column based on estimated affinity values\n",
    "df['label'] = df['Upper boundary for estimated affinity [nM]'].apply(categorize_activity)\n",
    "\n",
    "df.rename(columns={'USMILES': 'text'}, inplace=True)\n",
    "\n",
    "df['IC50'] = df['Upper boundary for estimated affinity [nM]']\n",
    "df = df[['text', 'label', 'IC50']]\n",
    "\n",
    "df.to_csv('mols2_IC50.csv', index=False)\n",
    "\n",
    "# Drop unneccessary columns\n",
    "df.drop(columns=['IC50'], inplace=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "# Save the modified DataFrame back to a csv file\n",
    "df.to_csv('mols2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Initial model variation selection training:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose pre-trained model for comparing the pre-training objectives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DeepChem/ChemBERTa-5M-MLM\"\n",
    "# model_name = \"DeepChem/ChemBERTa-5M-MTR\"\n",
    "# model_name = \"UdS-LSV/smole-bert-mtr\"\n",
    "# model_name = \"UdS-LSV/smole-bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or comparing dataset variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "model_name = \"seyonec/ChemBERTa-zinc250k-v1\"\n",
    "model_name = \"seyonec/PubChem10M_SMILES_BPE_60k\"\n",
    "\n",
    "model_name = \"DeepChem/ChemBERTa-5M-MLM\"\n",
    "model_name = \"DeepChem/ChemBERTa-10M-MLM\"\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the training, validation, and testing datasets used, as well as a run name for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "run_name = \"model 1 dataset 1 run 1\"\n",
    "\n",
    "train_df = pd.read_csv(\"traindata0/train_data_0.csv\")\n",
    "test_df = pd.read_csv(\"traindata0/test_data_0.csv\")\n",
    "val_df = pd.read_csv(\"traindata0/val_data_0.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata0/train_data_1.csv\")\n",
    "# test_df = pd.read_csv(\"traindata0/test_data_1.csv\")\n",
    "# val_df = pd.read_csv(\"traindata0/val_data_1.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata0/train_data_2.csv\")\n",
    "# test_df = pd.read_csv(\"traindata0/test_data_2.csv\")\n",
    "# val_df = pd.read_csv(\"traindata0/val_data_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenise the data and fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_smiles(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Prepare testing dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Prepare validation dataset\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "val_dataset = val_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "\n",
    "# Load pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, \n",
    "                                                        average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n_________\" + run_name + \" TRAINING FINISHED____________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluation of the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluation results:\")\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "test_labels = test_dataset[\"label\"]\n",
    "test_preds = predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "# Print some data samples along with their corresponding labels and predictions\n",
    "for i in range(5):\n",
    "    print(\"Data:\", test_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", test_labels[i])\n",
    "    print(\"Predicted Label:\", test_preds[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Fine-tuning and evaluation of different pre-trained models (using chosen variations):<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose and define the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"seyonec/ChemBERTa-zinc250k-v1\"\n",
    "# model_name = \"DeepChem/ChemBERTa-5M-MLM\"\n",
    "# model_name = \"jonghyunlee/ChemBERT_ChEMBL_pretrained\"\n",
    "# model_name = \"UdS-LSV/smole-bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Choose the datasets used and define the run name for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "run_name = \"model 1 data 1 run 1\"\n",
    "\n",
    "train_df = pd.read_csv(\"traindata1/train_data_0.csv\")\n",
    "test_df = pd.read_csv(\"traindata1/test_data_0.csv\")\n",
    "val_df = pd.read_csv(\"traindata1/val_data_0.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata1/train_data_1.csv\")\n",
    "# test_df = pd.read_csv(\"traindata1/test_data_1.csv\")\n",
    "# val_df = pd.read_csv(\"traindata1/val_data_1.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata1/train_data_2.csv\")\n",
    "# test_df = pd.read_csv(\"traindata1/test_data_2.csv\")\n",
    "# val_df = pd.read_csv(\"traindata1/val_data_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenise the data and fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_smiles(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Prepare testing dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Prepare validation dataset\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "val_dataset = val_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Load pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n_________\" + run_name + \" TRAINING FINISHED____________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Evaluation results:\")\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "test_labels = test_dataset[\"label\"]\n",
    "test_preds = predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "# Print some data samples along with their corresponding labels and predictions\n",
    "for i in range(5):  \n",
    "    print(\"Data:\", test_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", test_labels[i])\n",
    "    print(\"Predicted Label:\", test_preds[i])\n",
    "    print()\n",
    "\n",
    "if not os.path.exists(\"./predictions/\"):\n",
    "    os.makedirs(\"./predictions/\")\n",
    "\n",
    "with open(\"./predictions/\" + run_name.replace(\" \", \"\") + \"_predictions.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"Text\", \"True Label\", \"Predicted Label\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write predictions for each example in the test set\n",
    "    for i in range(len(predictions.predictions)):\n",
    "        writer.writerow({\n",
    "            \"Text\": test_dataset[i][\"text\"],\n",
    "            \"True Label\": test_labels[i],\n",
    "            \"Predicted Label\": test_preds[i]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Processing and evaluation of the collected model prediction data:<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "prediction_folder = \"./predictions/\"\n",
    "ic50_folder =  prediction_folder + \"withIC50/\"\n",
    "processed_folder = ic50_folder + \"processed/\"\n",
    "\n",
    "original_data_df = pd.read_csv(\"traindata/train_with_IC50.csv\")\n",
    "\n",
    "# Create a dictionary mapping text to IC50 values\n",
    "text_ic50_map = dict(zip(original_data_df[\"text\"], original_data_df[\"IC50\"]))\n",
    "\n",
    "# Function to retrieve IC50 value based on text\n",
    "def get_ic50(text):\n",
    "    return text_ic50_map.get(text, \"\")\n",
    "\n",
    "# Apply processing to every file in predictions directory\n",
    "for file_name in os.listdir(prediction_folder):\n",
    "    if file_name.endswith(\"_predictions.csv\"):\n",
    "        # Extract run_name from file_name\n",
    "        run_name = file_name.split(\"_predictions.csv\")[0]\n",
    "        predictions_df = pd.read_csv(prediction_folder + run_name + \"_predictions.csv\")\n",
    "        # Save predictions with IC50 values\n",
    "        predictions_df[\"IC50\"] = predictions_df[\"Text\"].apply(get_ic50)\n",
    "\n",
    "        if not os.path.exists(ic50_folder):\n",
    "            os.makedirs(ic50_folder)\n",
    "\n",
    "        predictions_df.to_csv(ic50_folder + run_name + \"_predictions_with_ic50.csv\", index=False)\n",
    "\n",
    "\n",
    "# Function to calculate the difference between IC50 and label bound\n",
    "def calculate_ic50_difference(row):\n",
    "    bound = 5000 if (row['Predicted Label'] == 0 or (row['Predicted Label'] == 1 and row[\"True Label\"] == 0)) else 250\n",
    "    return abs(int(row['IC50']) - bound)\n",
    "    \n",
    "\n",
    "for file_name in os.listdir(ic50_folder):\n",
    "    if file_name.endswith(\"_with_ic50.csv\"):\n",
    "        df = pd.read_csv(ic50_folder + file_name)\n",
    "\n",
    "        # Filter rows where predicted label does not match real label\n",
    "        mismatched_df = df[df['Predicted Label'] != df['True Label']].copy()\n",
    "\n",
    "        # Calculate IC50 difference\n",
    "        differences = mismatched_df.apply(calculate_ic50_difference, axis=1)\n",
    "        mismatched_df['IC50 Difference'] = differences.tolist()\n",
    "\n",
    "        # Group by model\n",
    "        model_number = file_name.split(\"l\")[1][0]\n",
    "\n",
    "        if not os.path.exists(processed_folder):\n",
    "            os.makedirs(processed_folder)\n",
    "\n",
    "        output_file_path = processed_folder + \"model\" + model_number + \"_predictions.csv\"\n",
    "\n",
    "        # Append to the existing file if it exists, otherwise create a new file\n",
    "        if os.path.exists(output_file_path):\n",
    "            mismatched_df.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            mismatched_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "for file_name in os.listdir(processed_folder):\n",
    "    if file_name.endswith(\"_predictions.csv\"):\n",
    "        df = pd.read_csv(processed_folder + file_name)\n",
    "\n",
    "        df[\"Duplicate Count\"] = df.groupby(df.columns[0]).transform('size').tolist()\n",
    "        model_df = df.drop_duplicates(keep='first').copy()\n",
    "        model_df.to_csv(processed_folder + file_name.split(\".\")[0] + \"_final.csv\", index=False)\n",
    "\n",
    "        summary_df = pd.DataFrame(columns=['Model', 'Incorrect Predictions', 'Off by Two', 'Relative Grade'])\n",
    "\n",
    "        summary = {\n",
    "            'Model': file_name.split(\"l\")[1][0],\n",
    "            'Incorrect Predictions': len(df),\n",
    "            'Off by Two': sum(abs(df['True Label'] - df['Predicted Label']) > 1),\n",
    "            'Relative Grade': sum((model_df['IC50 Difference'] / (model_df['IC50'] + model_df['IC50 Difference'])) * model_df['Duplicate Count']) / 100,\n",
    "        }\n",
    "        summary_df.loc[1] = summary\n",
    "\n",
    "        if os.path.exists(processed_folder + \"model_summary.csv\"):\n",
    "            summary_df.to_csv(processed_folder + \"model_summary.csv\", mode='a', index=False, header=False)\n",
    "        else:\n",
    "            summary_df.to_csv(processed_folder + \"model_summary.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Fine-tuning the final model:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without parameter optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and define the pre-trained model, specifying the respective number of labels used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1.1. For three labels:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"seyonec/ChemBERTa-zinc250k-v1\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1.2 For two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"seyonec/ChemBERTa-zinc250k-v1\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Choose and load the dataset combination for training and evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">2.1 For three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"traindata/train_shuffled.csv\")\n",
    "test_df = pd.read_csv(\"testdata/valdata.csv\")\n",
    "val_df = pd.read_csv(\"testdata/testdata.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata/train_withduplicates.csv\")\n",
    "# test_df = pd.read_csv(\"testdata/valdata.csv\")\n",
    "# val_df = pd.read_csv(\"testdata/testdata.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata/final_train.csv\")\n",
    "# test_df = pd.read_csv(\"testdata/final_val.csv\")\n",
    "# val_df = pd.read_csv(\"testdata/all_generated.csv\")\n",
    "\n",
    "mol_df = pd.read_csv(\"mols.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">2.2 For two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"traindata/train2_shuffled.csv\")\n",
    "test_df = pd.read_csv(\"data3/valdata2.csv\")\n",
    "val_df = pd.read_csv(\"data3/testdata2.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata/train2_withduplicates.csv\")\n",
    "# test_df = pd.read_csv(\"testdata/valdata2.csv\")\n",
    "# val_df = pd.read_csv(\"testdata/testdata2.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"traindata/final_train2.csv\")\n",
    "# test_df = pd.read_csv(\"traindata/final_val2.csv\")\n",
    "# val_df = pd.read_csv(\"testdata/all_generated2.csv\")\n",
    "\n",
    "mol_df = pd.read_csv(\"mols2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the compute_metrics method for handling the different number of labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">3.1 For three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">3.2 For two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tokenise the data and fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_smiles(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Prepare testing dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Prepare validation dataset\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "val_dataset = val_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "mol_dataset = Dataset.from_pandas(mol_df)\n",
    "mol_dataset = mol_dataset.map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results1',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n_________\" + run_name + \" TRAINING FINISHED____________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate the fine-tuned model on testing data with respective techniques for different numbers of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">5.1 Three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation results:\")\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "test_labels = test_dataset[\"label\"]\n",
    "test_preds = predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1, \"\\n\")\n",
    "\n",
    "# Print some data samples along with their corresponding labels and predictions\n",
    "for i in range(5):\n",
    "    print(\"Data:\", test_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", test_labels[i])\n",
    "    print(\"Predicted Label:\", test_preds[i])\n",
    "    print()\n",
    "\n",
    "mol_predictions = trainer.predict(mol_dataset)\n",
    "\n",
    "mol_labels = mol_dataset[\"label\"]\n",
    "mol_preds = mol_predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(mol_labels, mol_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(mol_labels, mol_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"Data:\", mol_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", mol_labels[i])\n",
    "    print(\"Predicted Label:\", mol_preds[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">5.2 Two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation results:\")\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "test_labels = test_dataset[\"label\"]\n",
    "test_preds = predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='binary')\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1, \"\\n\")\n",
    "\n",
    "# Print some data samples along with their corresponding labels and predictions\n",
    "for i in range(5):\n",
    "    print(\"Data:\", test_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", test_labels[i])\n",
    "    print(\"Predicted Label:\", test_preds[i])\n",
    "    print()\n",
    "\n",
    "mol_predictions = trainer.predict(mol_dataset)\n",
    "\n",
    "mol_labels = mol_dataset[\"label\"]\n",
    "mol_preds = mol_predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(mol_labels, mol_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(mol_labels, mol_preds, average='binary', zero_division=1)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "for i in range(9):\n",
    "    print(\"Data:\", mol_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", mol_labels[i])\n",
    "    print(\"Predicted Label:\", mol_preds[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Model fine-tuning with hyperparameter optimisation:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the model, import and tokenize datasets, and define compute_metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "model_name = \"seyonec/ChemBERTa-zinc250k-v1\"\n",
    "\n",
    "train_df = pd.read_csv(\"traindata/train2_shuffled.csv\")\n",
    "val_df = pd.read_csv(\"testdata/valdata2.csv\")\n",
    "test_df = pd.read_csv(\"testdata/testdata2.csv\")\n",
    "\n",
    "mol_df = pd.read_csv(\"mols2.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_smiles(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding=True)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize_smiles, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_smiles, batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize_smiles, batched=True)\n",
    "mol_dataset = Dataset.from_pandas(mol_df).map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenise the data, and fine-tune the model with hyperparameter search, or without for comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">2.1. Fine-tuning with hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "import optuna\n",
    "\n",
    "\n",
    "# Method for model initialization\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Hyperparameter search space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 15),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer with hyperparameter search\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=10,\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters found:\", best_run)\n",
    "\n",
    "print(\"\\n_________\" + run_name + \" TRAINING FINISHED____________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">2.2. Fine-tuning without hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results1',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n_________\" + run_name + \" TRAINING FINISHED____________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate the fine-tuned model using the testing datasets, and for each dataset produce the prediction accuracy, F1, precision-recall curve, and confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "test_labels = test_dataset[\"label\"]\n",
    "test_preds = predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='binary')\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(test_labels, predictions.predictions[:, 1])\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_curve, precision_curve, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1, \"\\n\")\n",
    "\n",
    "# Print some data samples along with their corresponding labels and predictions\n",
    "for i in range(5): \n",
    "    print(\"Data:\", test_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", test_labels[i])\n",
    "    print(\"Predicted Label:\", test_preds[i])\n",
    "    print()\n",
    "\n",
    "mol_predictions = trainer.predict(mol_dataset)\n",
    "\n",
    "mol_labels = mol_dataset[\"label\"]\n",
    "mol_preds = mol_predictions.predictions.argmax(axis=1)\n",
    "accuracy = accuracy_score(mol_labels, mol_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(mol_labels, mol_preds, average='binary', zero_division=1)\n",
    "\n",
    "# Compute precision-recall curve for mols set\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(mol_labels, mol_predictions.predictions[:, 1])\n",
    "\n",
    "# Plot precision-recall curve for mols set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_curve, precision_curve, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Mols Set')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for mols set\n",
    "cm = confusion_matrix(mol_labels, mol_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Mols Set')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "for i in range(9): \n",
    "    print(\"Data:\", mol_dataset[i][\"text\"])\n",
    "    print(\"True Label:\", mol_labels[i])\n",
    "    print(\"Predicted Label:\", mol_preds[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Final model training and saving:<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# Load datasets as DFs\n",
    "train_df = pd.read_csv('traindata/train2_shuffled.csv')\n",
    "mols_df = pd.read_csv('mols2.csv')\n",
    "\n",
    "# Combine and shuffle datasets\n",
    "combined_df = pd.concat([train_df, mols_df])\n",
    "shuffled_df = shuffle(combined_df).reset_index(drop=True)\n",
    "\n",
    "shuffled_df.to_csv('finalset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "run_name = \"ChemBERTaNLRP3\"\n",
    "model_name = \"seyonec/ChemBERTa-zinc250k-v1\"\n",
    "\n",
    "train_df = pd.read_csv(\"finalset.csv\")\n",
    "val_df = pd.read_csv(\"testdata/valdata2.csv\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_smiles(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding=True)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize_smiles, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_smiles, batched=True)\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finalresults',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./finallogs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    run_name=run_name,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n_________\" + run_name + \" TRAINING FINISHED____________\\n\")\n",
    "\n",
    "trainer.save_model('models/')\n",
    "model.save_pretrained('models/' + run_name, safe_serialization=False)\n",
    "tokenizer.save_pretrained('models/' + run_name, safe_serialization=False)\n",
    "\n",
    "print(\"Model and tokenizer saved as: \" + run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Published model use: <h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "model_name = \"VitaRin/ChemBERTaNLRP3\"\n",
    "\n",
    "pipeline = TextClassificationPipeline(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "    device=0\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\"mols2.csv\")\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "molecules = list(test_dataset[\"text\"])\n",
    "\n",
    "result = pipeline(molecules)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Mean calculator:<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For calculating the means of values with standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = int(input(\"How many numbers to take an average from?:\\n\"))\n",
    "numbers = []\n",
    "\n",
    "for i in range (choice):\n",
    "    number = float(input(\"Enter a number:\\n\"))\n",
    "    numbers.append(number)\n",
    "\n",
    "# Calculate the mean\n",
    "mean = sum(numbers) / len(numbers)\n",
    "\n",
    "# Calculate the sum of squares of differences\n",
    "sum_of_squares = sum((x - mean) ** 2 for x in numbers)\n",
    "\n",
    "# Calculate the variance and standard deviation\n",
    "variance = sum_of_squares / len(numbers)\n",
    "std_deviation = variance ** 0.5\n",
    "\n",
    "print(\"Average of\", numbers, \":\\n\")\n",
    "print(\"Average:\", round(mean, 4))\n",
    "print(\"Standard Deviation:\", round(std_deviation, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For calculating the average mean and standard deviation between mean values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Function to calculate mean of means and combined standard deviation\n",
    "def mean_of_means(means, std_devs):\n",
    "    # Calculate the combined mean and standard deviation\n",
    "    combined_mean = sum(means) / len(means)\n",
    "    combined_variance = sum((std_dev ** 2) for std_dev in std_devs) / len(means)\n",
    "    combined_std_dev = math.sqrt(combined_variance)\n",
    "\n",
    "    return combined_mean, combined_std_dev\n",
    "\n",
    "# Input number of means\n",
    "num_means = int(input(\"How many means to combine?:\\n\"))\n",
    "means, std_devs = [], []\n",
    "\n",
    "for i in range(num_means):\n",
    "    mean = float(input(f\"Enter mean value {i+1}:\\n\"))\n",
    "    std_dev = float(input(f\"Enter standard deviation for mean value {i+1}:\\n\"))\n",
    "    means.append(mean)\n",
    "    std_devs.append(std_dev)\n",
    "\n",
    "combined_mean, combined_std_dev = mean_of_means(means, std_devs)\n",
    "\n",
    "print(\"Combined Mean Average:\", round(combined_mean, 4))\n",
    "print(\"Combined Standard Deviation:\", round(combined_std_dev, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
